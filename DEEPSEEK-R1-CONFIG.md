# DeepSeek R1 é…ç½®æŒ‡å—

## âœ… é…ç½®å®ŒæˆçŠ¶æ€

- âœ… OllamaæœåŠ¡å·²é…ç½®åˆ°Docker Compose
- âœ… DeepSeek R1 (1.5B) æ¨¡å‹å·²ä¸‹è½½æˆåŠŸ
- âœ… æ¨¡å‹æµ‹è¯•æ­£å¸¸è¿è¡Œï¼Œæ”¯æŒä¸­æ–‡å¯¹è¯

## ğŸ“‹ å½“å‰é…ç½®

### Docker ComposeæœåŠ¡é…ç½®

```yaml
ollama:
  image: ollama/ollama:latest
  container_name: reading-ollama
  ports:
    - "11434:11434"
  volumes:
    - ollama_data:/root/.ollama
  environment:
    - OLLAMA_HOST=0.0.0.0
    - OLLAMA_ORIGINS=*
    - OLLAMA_NUM_PARALLEL=2
    - OLLAMA_MAX_LOADED_MODELS=1
    - OLLAMA_KEEP_ALIVE=5m
  networks:
    - reading-network
  restart: unless-stopped
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    interval: 30s
    timeout: 15s
    retries: 5
    start_period: 30s
```

### æ¨¡å‹ä¿¡æ¯

```
åç§°: deepseek-r1:1.5b
ID: e0979632db5a
å¤§å°: 1.1 GB
ç±»å‹: æ¨ç†ä¼˜åŒ–æ¨¡å‹
ç‰¹ç‚¹: DeepSeek R1è’¸é¦ç‰ˆæœ¬ï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†
```

## ğŸš€ ä½¿ç”¨æŒ‡å—

### 1. å¯åŠ¨æœåŠ¡

```bash
cd deployments/docker
docker compose up -d ollama
```

### 2. éªŒè¯æœåŠ¡çŠ¶æ€

```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker compose ps | grep ollama

# æ£€æŸ¥APIæ˜¯å¦å¯ç”¨
curl http://localhost:11434/api/version

# æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡å‹
docker exec reading-ollama ollama list
```

### 3. ç›´æ¥ä½¿ç”¨æ¨¡å‹

```bash
# äº¤äº’å¼ä½¿ç”¨
docker exec -it reading-ollama ollama run deepseek-r1:1.5b

# å•æ¬¡æŸ¥è¯¢
docker exec reading-ollama ollama run deepseek-r1:1.5b "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹è‡ªå·±"
```

### 4. åœ¨Difyä¸­é…ç½®ä½¿ç”¨

#### æ­¥éª¤1ï¼šæ·»åŠ æ¨¡å‹ä¾›åº”å•†
1. è®¿é—® Dify Webç•Œé¢ï¼šhttp://localhost:8093
2. ç™»å½•ç®¡ç†æ§åˆ¶å°
3. è¿›å…¥ **è®¾ç½®** â†’ **æ¨¡å‹ä¾›åº”å•†**
4. ç‚¹å‡» **+ æ·»åŠ ä¾›åº”å•†**
5. é€‰æ‹© **OpenAIå…¼å®¹**

#### æ­¥éª¤2ï¼šé…ç½®DeepSeek R1
```
ä¾›åº”å•†åç§°: DeepSeek-R1-Local
API Base URL: http://ollama:11434/v1
API Key: deepseek-r1
```

#### æ­¥éª¤3ï¼šæ·»åŠ æ¨¡å‹
```
æ¨¡å‹åç§°: deepseek-r1:1.5b
æ¨¡å‹ç±»å‹: æ–‡æœ¬ç”Ÿæˆ
ä¸Šä¸‹æ–‡é•¿åº¦: 4096
æœ€å¤§è¾“å‡ºtoken: 2048
æ”¯æŒè§†è§‰: å¦
```

## ğŸ¯ æ¨¡å‹ç‰¹æ€§

### DeepSeek R1 ç‰¹ç‚¹
- **æ¨ç†èƒ½åŠ›å¼º**ï¼šåŸºäºDeepSeek-R1æ¶æ„ï¼Œæ”¯æŒå¤æ‚æ¨ç†
- **ä¸­æ–‡å‹å¥½**ï¼šå¯¹ä¸­æ–‡ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ä¼˜ç§€
- **æ€ç»´é“¾æ¨ç†**ï¼šæ”¯æŒstep-by-stepæ€ç»´è¿‡ç¨‹
- **è½»é‡åŒ–**ï¼š1.5Bå‚æ•°ç‰ˆæœ¬ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²
- **é«˜æ•ˆç‡**ï¼šç»è¿‡è’¸é¦ä¼˜åŒ–ï¼Œæ¨ç†é€Ÿåº¦å¿«

### é€‚ç”¨åœºæ™¯
- é€»è¾‘æ¨ç†å’Œæ•°å­¦é—®é¢˜
- ä»£ç åˆ†æå’Œç”Ÿæˆ
- ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†
- é—®ç­”ç³»ç»Ÿ
- æ–‡æ¡£ç†è§£å’Œæ€»ç»“

## ğŸ”§ é«˜çº§é…ç½®

### GPUåŠ é€Ÿï¼ˆå¯é€‰ï¼‰

å¦‚æœä½ æœ‰NVIDIA GPUï¼Œå¯ä»¥å¯ç”¨GPUåŠ é€Ÿï¼š

1. **å–æ¶ˆæ³¨é‡ŠGPUé…ç½®**ï¼š
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: all
             capabilities: [gpu]
   ```

2. **é‡å¯æœåŠ¡**ï¼š
   ```bash
   docker compose down ollama
   docker compose up -d ollama
   ```

### æ€§èƒ½è°ƒä¼˜

```bash
# è®¾ç½®æ¨¡å‹ä¿æŒæ—¶é—´ï¼ˆå‡å°‘é‡å¤åŠ è½½ï¼‰
docker exec reading-ollama ollama run deepseek-r1:1.5b --keep-alive 10m

# é¢„åŠ è½½æ¨¡å‹åˆ°å†…å­˜
docker exec reading-ollama curl -X POST http://localhost:11434/api/generate \
  -d '{"model": "deepseek-r1:1.5b", "prompt": "", "keep_alive": -1}'
```

## ğŸ“¡ APIä½¿ç”¨ç¤ºä¾‹

### ç›´æ¥APIè°ƒç”¨

```bash
# åŸºç¡€å¯¹è¯
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-r1:1.5b",
    "messages": [
      {"role": "user", "content": "è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

### é€šè¿‡Dify API

```bash
curl -X POST 'http://localhost:8091/v1/chat-messages' \
  --header 'Authorization: Bearer app-xxxxxxxxxx' \
  --header 'Content-Type: application/json' \
  --data-raw '{
    "inputs": {},
    "query": "å¸®æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªç®—æ³•çš„å¤æ‚åº¦",
    "response_mode": "blocking",
    "conversation_id": "",
    "user": "reading-user"
  }'
```

## ğŸ› ï¸ ç®¡ç†ç»´æŠ¤

### æ¨¡å‹ç®¡ç†

```bash
# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
docker exec reading-ollama ollama show deepseek-r1:1.5b

# æ›´æ–°æ¨¡å‹
docker exec reading-ollama ollama pull deepseek-r1:1.5b

# åˆ é™¤æ¨¡å‹ï¼ˆå¦‚éœ€è¦ï¼‰
docker exec reading-ollama ollama rm deepseek-r1:1.5b
```

### ç›‘æ§å’Œæ—¥å¿—

```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker compose ps ollama

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker logs reading-ollama -f

# ç›‘æ§èµ„æºä½¿ç”¨
docker stats reading-ollama
```

### æ•°æ®å¤‡ä»½

```bash
# å¤‡ä»½æ¨¡å‹æ•°æ®
docker run --rm -v reading-microservices_ollama_data:/data \
  -v $(pwd):/backup ubuntu \
  tar czf /backup/deepseek-r1-backup.tar.gz /data
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹å“åº”æ…¢**
   - æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
   - è€ƒè™‘å¯ç”¨GPUåŠ é€Ÿ
   - è°ƒæ•´OLLAMA_KEEP_ALIVEå‚æ•°

2. **å†…å­˜ä¸è¶³**
   - ç¡®ä¿è‡³å°‘æœ‰3GBå¯ç”¨å†…å­˜
   - å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„åº”ç”¨
   - ä½¿ç”¨swapåˆ†åŒºä½œä¸ºè¡¥å……

3. **ç½‘ç»œè¿æ¥é—®é¢˜**
   - æ£€æŸ¥ç«¯å£11434æ˜¯å¦è¢«å ç”¨
   - éªŒè¯Dockerç½‘ç»œé…ç½®
   - æµ‹è¯•APIè¿é€šæ€§

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

```yaml
# é’ˆå¯¹DeepSeek R1çš„ä¼˜åŒ–ç¯å¢ƒå˜é‡
environment:
  - OLLAMA_HOST=0.0.0.0
  - OLLAMA_ORIGINS=*
  - OLLAMA_NUM_PARALLEL=1      # 1.5Bæ¨¡å‹å»ºè®®å•çº¿ç¨‹
  - OLLAMA_MAX_LOADED_MODELS=1 # åªä¿æŒä¸€ä¸ªæ¨¡å‹åœ¨å†…å­˜
  - OLLAMA_KEEP_ALIVE=10m      # å»¶é•¿æ¨¡å‹ä¿æŒæ—¶é—´
```

## ğŸ“š ç›¸å…³èµ„æº

- **DeepSeekå®˜ç½‘**: https://www.deepseek.com/
- **DeepSeek R1è®ºæ–‡**: https://arxiv.org/abs/2501.12948
- **Ollamaæ–‡æ¡£**: https://ollama.com/docs
- **æ¨¡å‹è¯¦æƒ…**: https://ollama.com/deepseek/deepseek-r1

## ğŸ‰ æµ‹è¯•éªŒè¯

æ¨¡å‹å·²é€šè¿‡ä»¥ä¸‹æµ‹è¯•ï¼š
- âœ… åŸºç¡€å¯¹è¯åŠŸèƒ½
- âœ… ä¸­æ–‡ç†è§£å’Œç”Ÿæˆ
- âœ… æ¨ç†æ€ç»´é“¾è¾“å‡º
- âœ… APIæ¥å£è°ƒç”¨
- âœ… Dockerå®¹å™¨ç¨³å®šæ€§

DeepSeek R1æ¨¡å‹ç°å·²å®Œå…¨é…ç½®å°±ç»ªï¼Œå¯ä»¥åœ¨ä½ çš„é˜…è¯»å¾®æœåŠ¡ç³»ç»Ÿä¸­ä½¿ç”¨ï¼